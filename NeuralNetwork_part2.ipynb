{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二階段-承作業階段一，資料集改成XOR的資料集，且類神經大小定義為2-2-1\n",
    "1. 完成這個資料集的分類動作。在這個資料集中，你會產生10000筆虛擬資料 <br>\n",
    "2. 輸入範圍落在[-0.5, 0.2]或[0.8, 1.5]兩個區塊中<br>\n",
    "3. 資料落在[-0.5, 0.2]中，代表其會被轉為0，反之若落在[0.8, 1.5]中，代表其會被轉為1<br>\n",
    "4. 每個虛擬資料最後屬於哪個類別則由XOR邏輯決定<br>\n",
    "* PS1: 類神經的輸入是x與y的數值，輸出則是一個數值，若此數值小於等於0.5，則被分到第1類，反之若大於0.5，則被分到第2類。這個類神經的輸入與輸出都要先完成正規化後才能開始類神經的訓練 <br>\n",
    "* PS2: 請把每次訓練的參數結果視覺化呈現出來(能做成動態更佳，如下面的影片所示)，並嘗試解讀這樣的視覺化結果<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 初始值\n",
    "def init(n):\n",
    "    global W2, W1, b1, b2, neuron_num, learning_rate\n",
    "    ## 類神經網路\n",
    "    # 隱藏層神經元數\n",
    "    neuron_num = n\n",
    "\n",
    "    ## 權重與偏差\n",
    "    W1 = np.random.randn(neuron_num, 2)\n",
    "    W2 = np.random.randn(1, neuron_num)\n",
    "    b1 = np.random.randn(neuron_num)\n",
    "    b2 = np.random.randn(1)\n",
    "    learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 產生數據\n",
    "def generateData():\n",
    "    ## DATA\n",
    "    # 數據數量\n",
    "    data_num = 10000\n",
    "\n",
    "    # 輸入值: ([-0.5, 0.2]或[0.8, 1.5]兩個區塊中)二維陣列\n",
    "    X = np.random.uniform(-1, 1, size=(data_num, 2))\n",
    "    X[X >= 0] = 0.7 * X[X >= 0] + 0.8\n",
    "    X[X < 0] = 0.7 * X[X < 0] + 0.2\n",
    "    bool_X = np.copy(X)\n",
    "    bool_X[X >= 0.8] = 1\n",
    "    bool_X[X < 0.8] = 0\n",
    "\n",
    "    # 輸出: 1或0\n",
    "    Y = np.where(bool_X[:, 0] == bool_X[:, 1], 0, 1).reshape(-1, 1)\n",
    "\n",
    "    # 對輸入和輸出進行正規化\n",
    "    X, min_inputs, range_inputs = standardize(X)\n",
    "    Y, min_outputs, range_outputs = standardize(Y)\n",
    "    # 切成訓練與測試\n",
    "    train_ratio = 0.7\n",
    "    train_size = int(train_ratio * len(X))\n",
    "    train_X = X[:train_size]\n",
    "    train_Y = Y[:train_size]\n",
    "    test_X = X[train_size:]\n",
    "    test_Y = Y[train_size:]\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y, min_inputs, min_outputs,range_inputs, range_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 正規化\n",
    "def standardize(numArray):\n",
    "    # 正規化至0到1之間\n",
    "    min_val = np.min(numArray)\n",
    "    range_val = np.max(numArray) - min_val\n",
    "    numArr_std = (numArray - min_val) / range_val\n",
    "    return numArr_std, min_val, range_val\n",
    "# FUN: 反正規化\n",
    "def unstandardize(numArray, min_val, range_val):\n",
    "    return numArray * range_val + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUN: activation function\n",
    "# tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "# https://www.baeldung.com/cs/sigmoid-vs-tanh-functions\n",
    "def TangentSigmoid(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 正向傳播\n",
    "def forward(X0):\n",
    "    # 第一層\n",
    "    Z1 = np.dot(X0, W1.T) + b1\n",
    "    X1 = TangentSigmoid(Z1)\n",
    "\n",
    "    # 第二層\n",
    "    Z2 = np.dot(X1, W2.T) + b2\n",
    "    X2 = TangentSigmoid(Z2)\n",
    "\n",
    "    return Z1, X1, Z2, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 反向傳播\n",
    "# FUN: Tangent Sigmoid 微分\n",
    "def dTangentSigmoid(x):\n",
    "    return 1 - np.power(np.tanh(x), 2)\n",
    "# FUN: 計算輸出層梯度\n",
    "def delta_output(Z, Y):\n",
    "    return (TangentSigmoid(Z) - Y) * dTangentSigmoid(Z)\n",
    "# FUN: 計算隱藏層梯度\n",
    "def delta_hidden(Z, D, W):\n",
    "    return dTangentSigmoid(Z) * np.dot(D, W)\n",
    "# FUN: 倒傳遞\n",
    "def backward(Y, Z2, Z1):\n",
    "    # 計算輸出層梯度\n",
    "    D2 = delta_output(Z2, Y)\n",
    "\n",
    "    # 計算隱藏層梯度\n",
    "    D1 = delta_hidden(Z1, D2, W2)\n",
    "\n",
    "    return D2, D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 計算目標函數對權重的梯度\n",
    "def computeWeight(D, X):\n",
    "    return np.dot(D.T, X)\n",
    "\n",
    "# FUN: 計算目標函數對偏差的梯度\n",
    "def computeBias(D):\n",
    "    return D.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 參數更新\n",
    "def update_paras(D2, X1, D1, X0):\n",
    "    global W2, W1, b1, b2, neuron_num, learning_rate\n",
    "    # 更新權重\n",
    "    W2 = W2 - learning_rate * computeWeight(D2, X1)\n",
    "    W1 = W1 - learning_rate * computeWeight(D1, X0)\n",
    "\n",
    "    # 更新偏差\n",
    "    b2 = b2 - learning_rate * computeBias(D2)\n",
    "    b1 = b1 - learning_rate * computeBias(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUN: 學習\n",
    "def train(X, Y):\n",
    "    # 正向傳播\n",
    "    Z1, X1, Z2, X2 = forward(X)\n",
    "\n",
    "    # 反向傳播\n",
    "    D2, D1 = backward(Y, Z2, Z1)\n",
    "\n",
    "    # 參數更新\n",
    "    update_paras(D2, X1, D1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 預測\n",
    "def predict(X):\n",
    "    # -1為最後\n",
    "    ans = forward(X)[-1]\n",
    "    return ans\n",
    "\n",
    "# FUN: 計算誤差\n",
    "def error_function(Y, X):\n",
    "    # 1/2*error^2\n",
    "    a = predict(X)\n",
    "    return 0.5 * ((Y - a) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 執行\n",
    "def exec(times, max_n, epoch):\n",
    "    # 保存每次訓練的誤差\n",
    "    diffs = []  \n",
    "    epoch_list = []\n",
    "\n",
    "    # 重複訓練\n",
    "    for time in range(0, times):\n",
    "        train_X, train_Y, test_X, test_Y, min_inputs, min_outputs,range_inputs, range_outputs = generateData()\n",
    "\n",
    "        for n in range(2, max_n+1):\n",
    "            # NN\n",
    "            init(n)\n",
    "\n",
    "            ## 批次執行\n",
    "            batch = 100\n",
    "            for e in range(0, epoch):\n",
    "                # 隨機打亂訓練資料的索引\n",
    "                p = np.random.permutation(len(train_X))\n",
    "\n",
    "                # 遍歷每個batch\n",
    "                for i in range(math.ceil(len(train_X) / batch)):\n",
    "                    # 取出當前batch的索引範圍\n",
    "                    indice = p[i * batch:(i + 1) * batch]\n",
    "\n",
    "                    # 根據索引範圍從原始資料中取出對應的batch資料\n",
    "                    X = train_X[indice]\n",
    "                    Y = train_Y[indice]\n",
    "\n",
    "                    # 使用當前batch資料進行模型訓練\n",
    "                    train(X, Y)\n",
    "\n",
    "                # 預測並分類\n",
    "                preds = predict(test_X)\n",
    "                classify_preds = np.where(preds <= 0.5, 0, 1)\n",
    "                ## 記錄誤差\n",
    "                mean_diff = np.mean(np.abs(classify_preds - test_Y))\n",
    "                ##print(mean_diff)  \n",
    "                # 添加至視覺化列表\n",
    "                epoch_list.append(e + 1)\n",
    "                diffs.append(mean_diff)\n",
    "\n",
    "                # 繪製並保存圖片\n",
    "                plt.plot(epoch_list, diffs, '-o')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Mean Difference')\n",
    "                plt.title('Mean Difference vs. Epoch')\n",
    "                plt.xlim(1, epoch)\n",
    "                plt.ylim(0, 1)\n",
    "                # 加入每個點的標籤\n",
    "                #for x, y in zip(epoch_list, diffs):\n",
    "                #    plt.text(x, y, f'{y:.2f}', ha='center', va='bottom')\n",
    "                plt.savefig(f'epoch_{len(epoch_list)}.png')\n",
    "                plt.close()\n",
    "                \n",
    "    return diffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 執行\n",
    "times = 1\n",
    "max_n = 2\n",
    "epoch = 100\n",
    "exec(times, max_n, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將圖片組合為GIF動畫\n",
    "images = []\n",
    "for e in range(epoch):\n",
    "    images.append(imageio.imread(f'epoch_{e+1}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('dynamic_visualization.gif', images, duration=0.1)   \n",
    "\n",
    "for e in range(epoch):\n",
    "    os.remove(f'epoch_{e+1}.png')\n",
    "print('end')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
