{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一階段-撰寫一個2-n-1的淺層類神經網路\n",
    "1. 隱藏層的神經元使用Tangent Sigmoid作為激活函數 <br>\n",
    "2. 輸出層為線性函數<br>\n",
    "3. 類神經網路的輸入為兩個四位數的數值，輸出則為這兩個數的總和<br>\n",
    "4. 調整你的隱藏層神經元數量，並畫出一張神經元數量對30次訓練結果之誤差平均的圖。<br>\n",
    "* PS1: 撰寫類神經前請先產生你的資料集，建議要有10000筆以上，接著把這個資料集切開成訓練與測試資料集。* <br>\n",
    "* PS2: 不管是輸入還是輸出都要正規化到0-1之間，而產出結果時，需要把輸出反正規化到原本的空間中。*<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 初始值\n",
    "def init():\n",
    "    global W2, W1, b1, b2, neuron_num, learning_rate\n",
    "    ## 類神經網路\n",
    "    # 隱藏層神經元數\n",
    "    neuron_num = np.random.randint(2, 9)\n",
    "\n",
    "    ## 權重與偏差\n",
    "    W1 = np.random.randn(neuron_num, 2)\n",
    "    W2 = np.random.randn(1, neuron_num)\n",
    "    b1 = np.random.randn(neuron_num)\n",
    "    b2 = np.random.randn(1)\n",
    "    learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 正規化\n",
    "def standardize(numArray):\n",
    "    # 正規化至0到1之間\n",
    "    min_val = np.min(numArray)\n",
    "    range_val = np.max(numArray) - min_val\n",
    "    numArr_std = (numArray - min_val) / range_val\n",
    "    return numArr_std, min_val, range_val\n",
    "\n",
    "# FUN: 反正規化\n",
    "def unstandardize(numArray, min_val, range_val):\n",
    "    return numArray * range_val + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: activation function\n",
    "# tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "# https://www.baeldung.com/cs/sigmoid-vs-tanh-functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUN: activation function\n",
    "# tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "# https://www.baeldung.com/cs/sigmoid-vs-tanh-functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# FUN: 正向傳播\n",
    "def forward(X0):\n",
    "    # 第一層\n",
    "    Z1 = np.dot(X0, W1.T) + b1\n",
    "    X1 = sigmoid(Z1)\n",
    "\n",
    "    # 第二層\n",
    "    Z2 = np.dot(X1, W2.T) + b2\n",
    "    X2 = sigmoid(Z2)\n",
    "\n",
    "    return Z1, X1, Z2, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 反向傳播\n",
    "# FUN: Tangent Sigmoid 微分\n",
    "def dsigmoid(x): \n",
    "    return (1 - sigmoid(x)) * sigmoid(x)\n",
    "# FUN: 計算輸出層梯度\n",
    "def delta_output(Z, Y):\n",
    "    return (sigmoid(Z) - Y) * dsigmoid(Z)\n",
    "# FUN: 計算隱藏層梯度\n",
    "def delta_hidden(Z, D, W):\n",
    "    return dsigmoid(Z) * np.dot(D, W)\n",
    "# FUN: 倒傳遞\n",
    "def backward(Y, Z2, Z1):\n",
    "    # 計算輸出層梯度\n",
    "    D2 = delta_output(Z2, Y)\n",
    "\n",
    "    # 計算隱藏層梯度\n",
    "    D1 = delta_hidden(Z1, D2, W2)\n",
    "\n",
    "    return D2, D1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 目標函數對權重微分\n",
    "def deweight(D, X):\n",
    "    return np.dot(D.T, X)\n",
    "\n",
    "# FUN: 目標函數對偏差微分\n",
    "def debias(D):\n",
    "    return D.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 參數更新\n",
    "def update_paras(D2, X1, D1, X0):\n",
    "    global W2, W1, b1, b2, neuron_num, learning_rate\n",
    "    # 更新權重\n",
    "    W2 = W2 - learning_rate * deweight(D2, X1)\n",
    "    W1 = W1 - learning_rate * deweight(D1, X0)\n",
    "\n",
    "    # 更新偏差\n",
    "    b2 = b2 - learning_rate * debias(D2)\n",
    "    b1 = b1 - learning_rate * debias(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 執行\n",
    "# FUN: 預測\n",
    "def predict(X):\n",
    "    # -1為最後\n",
    "    ans = forward(X)[-1]\n",
    "    return ans\n",
    "\n",
    "# FUN: 目標函數\n",
    "def error_function(Y, X):\n",
    "    # 1/2*error^2\n",
    "    a = predict(X)\n",
    "    return 0.5 * ((Y - a) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA\n",
    "# 數據數量\n",
    "data_num = 12345\n",
    "# 輸入值: (四位數, 四位數)陣列\n",
    "TX = (np.random.rand(data_num, 2)*9998).astype(np.int32)+1\n",
    "# 輸出值: 四位數 + 四位數 陣列\n",
    "TY = np.sum(TX, axis=1).reshape(-1, 1)\n",
    "# 對輸入和輸出進行正規化\n",
    "TX, min_inputs, range_inputs = standardize(TX)\n",
    "TY, min_expecteds, range_expecteds = standardize(TY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 批次執行\n",
    "batch = 100\n",
    "epoch = 100\n",
    "for e in range(1, epoch + 1):\n",
    "    # 隨機打亂訓練資料的索引\n",
    "    p = np.random.permutation(len(TX))\n",
    "\n",
    "    # 遍歷每個batch\n",
    "    for i in range(math.ceil(len(TX) / batch)):\n",
    "        # 取出當前batch的索引範圍\n",
    "        indice = p[i * batch:(i + 1) * batch]\n",
    "\n",
    "        # 根據索引範圍從原始資料中取出對應的batch資料\n",
    "        X0 = TX[indice]\n",
    "        Y = TY[indice]\n",
    "\n",
    "        # 使用當前batch資料進行模型訓練\n",
    "        train(X0, Y)\n",
    "\n",
    "    # 輸出訓練誤差\n",
    "    if e % 1 == 0:\n",
    "        error = error_function(TY, TX)\n",
    "        log = f'\\\n",
    "        error = {error} ({e}th epoch),\\n \\\n",
    "        '\n",
    "        #W1:{lib.weights_L1},\\n \\\n",
    "        #W2:{lib.weights_L2},\\n \\\n",
    "        #b1{lib.bias_L1},\\n \\\n",
    "        #b2{lib.bias_L2}\\\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 對預測結果進行反正規化\n",
    "predicted_output = unstandardize(predict(TX), min_expecteds, range_expecteds)\n",
    "origin_TY = unstandardize(TY, min_expecteds, range_expecteds)\n",
    "mean_diff = np.mean(np.abs(predicted_output - origin_TY))\n",
    "print(mean_diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
