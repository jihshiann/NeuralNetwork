{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一階段-撰寫一個2-n-1的淺層類神經網路\n",
    "1. 隱藏層的神經元使用Tangent Sigmoid作為激活函數 <br>\n",
    "2. 輸出層為線性函數<br>\n",
    "3. 類神經網路的輸入為兩個四位數的數值，輸出則為這兩個數的總和<br>\n",
    "4. 調整你的隱藏層神經元數量，並畫出一張神經元數量對30次訓練結果之誤差平均的圖。<br>\n",
    "* PS1: 撰寫類神經前請先產生你的資料集，建議要有10000筆以上，接著把這個資料集切開成訓練與測試資料集。* <br>\n",
    "* PS2: 不管是輸入還是輸出都要正規化到0-1之間，而產出結果時，需要把輸出反正規化到原本的空間中。*<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 初始值\n",
    "def init(n):\n",
    "    global W2, W1, b1, b2, neuron_num, learning_rate\n",
    "    ## 類神經網路\n",
    "    # 隱藏層神經元數\n",
    "    neuron_num = n\n",
    "\n",
    "    ## 權重與偏差\n",
    "    W1 = np.random.randn(neuron_num, 2)\n",
    "    W2 = np.random.randn(1, neuron_num)\n",
    "    b1 = np.random.randn(neuron_num)\n",
    "    b2 = np.random.randn(1)\n",
    "    learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 產生數據\n",
    "def generateData():\n",
    "    ## DATA\n",
    "    # 數據數量\n",
    "    data_num = 10000\n",
    "    # 輸入值: (四位數, 四位數)陣列\n",
    "    X = np.random.randint(1111, 9999+1, size=(data_num, 2))\n",
    "    # 輸出值: 四位數 + 四位數 陣列\n",
    "    Y = np.sum(X, axis=1).reshape(-1, 1)\n",
    "\n",
    "    # 對輸入和輸出進行正規化\n",
    "    X, min_inputs, range_inputs = standardize(X)\n",
    "    Y, min_outputs, range_outputs = standardize(Y)\n",
    "    # 切成訓練與測試\n",
    "    train_ratio = 0.7\n",
    "    train_size = int(train_ratio * len(X))\n",
    "    train_X = X[:train_size]\n",
    "    train_Y = Y[:train_size]\n",
    "    test_X = X[train_size:]\n",
    "    test_Y = Y[train_size:]\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y, min_inputs, min_outputs,range_inputs, range_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 正規化\n",
    "def standardize(numArray):\n",
    "    # 正規化至0到1之間\n",
    "    min_val = np.min(numArray)\n",
    "    range_val = np.max(numArray) - min_val\n",
    "    numArr_std = (numArray - min_val) / range_val\n",
    "    return numArr_std, min_val, range_val\n",
    "\n",
    "# FUN: 反正規化\n",
    "def unstandardize(numArray, min_val, range_val):\n",
    "    return numArray * range_val + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUN: activation function\n",
    "# tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "# https://www.baeldung.com/cs/sigmoid-vs-tanh-functions\n",
    "def TangentSigmoid(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 正向傳播\n",
    "def forward(X0):\n",
    "    # 第一層\n",
    "    Z1 = np.dot(X0, W1.T) + b1\n",
    "    X1 = TangentSigmoid(Z1)\n",
    "\n",
    "    # 第二層\n",
    "    Z2 = np.dot(X1, W2.T) + b2\n",
    "    X2 = TangentSigmoid(Z2)\n",
    "\n",
    "    return Z1, X1, Z2, X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 反向傳播\n",
    "# FUN: Tangent Sigmoid 微分\n",
    "def dTangentSigmoid(x):\n",
    "    return 1 - np.power(np.tanh(x), 2)\n",
    "# FUN: 計算輸出層梯度\n",
    "def delta_output(Z, Y):\n",
    "    return (TangentSigmoid(Z) - Y) * dTangentSigmoid(Z)\n",
    "# FUN: 計算隱藏層梯度\n",
    "def delta_hidden(Z, D, W):\n",
    "    return dTangentSigmoid(Z) * np.dot(D, W)\n",
    "# FUN: 倒傳遞\n",
    "def backward(Y, Z2, Z1):\n",
    "    # 計算輸出層梯度\n",
    "    D2 = delta_output(Z2, Y)\n",
    "\n",
    "    # 計算隱藏層梯度\n",
    "    D1 = delta_hidden(Z1, D2, W2)\n",
    "\n",
    "    return D2, D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 目標函數對權重微分\n",
    "def deweight(D, X):\n",
    "    return np.dot(D.T, X)\n",
    "\n",
    "# FUN: 目標函數對偏差微分\n",
    "def debias(D):\n",
    "    return D.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 參數更新\n",
    "def update_paras(D2, X1, D1, X0):\n",
    "    global W2, W1, b1, b2, neuron_num, learning_rate\n",
    "    # 更新權重\n",
    "    W2 = W2 - learning_rate * deweight(D2, X1)\n",
    "    W1 = W1 - learning_rate * deweight(D1, X0)\n",
    "\n",
    "    # 更新偏差\n",
    "    b2 = b2 - learning_rate * debias(D2)\n",
    "    b1 = b1 - learning_rate * debias(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUN: 學習\n",
    "def train(X, Y):\n",
    "    # 正向傳播\n",
    "    Z1, X1, Z2, X2 = forward(X)\n",
    "\n",
    "    # 反向傳播\n",
    "    D2, D1 = backward(Y, Z2, Z1)\n",
    "\n",
    "    # 參數更新\n",
    "    update_paras(D2, X1, D1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUN: 預測\n",
    "def predict(X):\n",
    "    # -1為最後\n",
    "    ans = forward(X)[-1]\n",
    "    return ans\n",
    "\n",
    "# FUN: 計算誤差\n",
    "def error_function(Y, X):\n",
    "    # 1/2*error^2\n",
    "    a = predict(X)\n",
    "    return 0.5 * ((Y - a) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 執行\n",
    "def exec(times, max_n, epoch):\n",
    "    # 保存每次訓練的誤差\n",
    "    diffs = []  \n",
    "    epoch_list = []\n",
    "\n",
    "    # 重複訓練\n",
    "    for time in range(0, times):\n",
    "        train_X, train_Y, test_X, test_Y, min_inputs, min_outputs,range_inputs, range_outputs = generateData()\n",
    "\n",
    "        for n in range(1, max_n+1):\n",
    "            # NN\n",
    "            init(n)\n",
    "\n",
    "            ## 批次執行\n",
    "            batch = 100\n",
    "            for e in range(0, epoch):\n",
    "                # 隨機打亂訓練資料的索引\n",
    "                p = np.random.permutation(len(train_X))\n",
    "\n",
    "                # 遍歷每個batch\n",
    "                for i in range(math.ceil(len(train_X) / batch)):\n",
    "                    # 取出當前batch的索引範圍\n",
    "                    indice = p[i * batch:(i + 1) * batch]\n",
    "\n",
    "                    # 根據索引範圍從原始資料中取出對應的batch資料\n",
    "                    X = train_X[indice]\n",
    "                    Y = train_Y[indice]\n",
    "\n",
    "                    # 使用當前batch資料進行模型訓練\n",
    "                    train(X, Y)\n",
    "\n",
    "                 \n",
    "                # 輸出訓練誤差\n",
    "                if e % 10 == 0:\n",
    "                    error = error_function(train_Y, train_X)\n",
    "                    log = f'\\\n",
    "                    time = {time}, n = {n}, error = {error} ({e}th epoch),\\n \\\n",
    "                    '\n",
    "                    print(log)\n",
    "                ## 預測並反正規化\n",
    "                predicted_outputs = unstandardize(predict(test_X), min_outputs, range_outputs)\n",
    "                origin_outputs = unstandardize(test_Y, min_outputs, range_outputs)\n",
    "                ## 記錄誤差\n",
    "                mean_diff = np.mean(np.abs(predicted_outputs - origin_outputs))\n",
    "                if len(diffs) < n:\n",
    "                    diffs.append(mean_diff)\n",
    "                else:\n",
    "                    diffs[n-1] += mean_diff\n",
    "                \n",
    "    return diffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m max_n \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[0;32m      3\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m30\u001b[39m\n\u001b[1;32m----> 4\u001b[0m diffs \u001b[39m=\u001b[39m exec(times, max_n, epoch)\n\u001b[0;32m      5\u001b[0m mean_diffs \u001b[39m=\u001b[39m [diff \u001b[39m/\u001b[39m times \u001b[39mfor\u001b[39;00m diff \u001b[39min\u001b[39;00m diffs]\n",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m, in \u001b[0;36mexec\u001b[1;34m(times, max_n, epoch)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# 重複訓練\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m time \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, times):\n\u001b[1;32m----> 9\u001b[0m     train_X, train_Y, test_X, test_Y, min_inputs, min_outputs,range_inputs, range_outputs \u001b[39m=\u001b[39m generateData(stage)\n\u001b[0;32m     11\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m         \u001b[39m# NN\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         init(n)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stage' is not defined"
     ]
    }
   ],
   "source": [
    "times = 30\n",
    "max_n = 30\n",
    "epoch = 30\n",
    "diffs = exec(times, max_n, epoch)\n",
    "mean_diffs = [diff / times for diff in diffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 繪圖\n",
    "n_list = list(range(1, max_n+1))\n",
    "plt.plot(n_list, mean_diffs)\n",
    "plt.xlabel('Number of Neurons')\n",
    "plt.ylabel('Mean Diff')\n",
    "plt.title('Number of Neurons vs. Mean Diff')\n",
    "plt.show()\n",
    "print('end')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
